{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GraphMixer",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1_KAuFePMRq_aJ_OlOZtkoP8DiWLBhwm8",
      "authorship_tag": "ABX9TyMfGFJWtlkS2tkFRq2dpoad",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2019mohamed/GraphMixer/blob/main/GraphMixer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgWhQTSnZLUF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8822c63e-b4db-4a23-d614-ac1134b454e2"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops.layers.torch import Rearrange\n",
        "import networkx as nx\n",
        "import dgl\n",
        "import numpy as np\n",
        "\n",
        "class GlobalAveragePooling(nn.Module):\n",
        "\n",
        "    def __init__(self, dim: int = 1):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return x.mean(dim=self.dim)\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.model = nn.Linear(input_dim, num_classes)\n",
        "        nn.init.zeros_(self.model.weight)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class MLPBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class MixerBlock(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_patches: int,\n",
        "        num_channels: int,\n",
        "        tokens_hidden_dim: int,\n",
        "        channels_hidden_dim: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.token_mixing = nn.Sequential(\n",
        "            nn.LayerNorm(num_channels),\n",
        "            Rearrange(\"b p c -> b c p\"),\n",
        "            MLPBlock(num_patches, tokens_hidden_dim),\n",
        "            Rearrange(\"b c p -> b p c\")\n",
        "        )\n",
        "        self.channel_mixing = nn.Sequential(\n",
        "            nn.LayerNorm(num_channels),\n",
        "            MLPBlock(num_channels, channels_hidden_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x + self.token_mixing(x)\n",
        "        x = x + self.channel_mixing(x)\n",
        "        return x\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self,\n",
        "        num_layers:int,\n",
        "        num_patches: int,\n",
        "        num_channels: int,\n",
        "        tokens_hidden_dim: int,\n",
        "        channels_hidden_dim: int,\n",
        "        num_classes: int):\n",
        "        super().__init__()\n",
        "        self.layers = [MixerBlock(num_patches , num_channels , tokens_hidden_dim , channels_hidden_dim) for _ in range(num_layers)]\n",
        "        self.mixers = nn.Sequential(*self.layers)\n",
        "        self.pool = GlobalAveragePooling()\n",
        "        self.classifier = Classifier(num_channels ,num_classes)\n",
        "\n",
        "    def forward (self,x):\n",
        "      out = self.mixers(x)\n",
        "      pooled = self.pool(out)\n",
        "      return self.classifier(pooled)\n",
        "\n",
        "\n",
        "from dgl.data import GINDataset\n",
        "from dgl.nn.pytorch.conv import GraphConv, SAGEConv\n",
        "from dgl.nn import SumPooling, AvgPooling, MaxPooling\n",
        "dataset = GINDataset('PROTEINS' , self_loop= True)\n",
        "#print(len(dataset[0][0].ndata['attr']),' ',dataset[0])\n",
        "perm = np.random.permutation(len(dataset))\n",
        "train_idx = perm[:1001]\n",
        "test_idx = perm[1001:]\n",
        "\n",
        "train_emd , test_emd = [], []\n",
        "train_label , test_label = [],[]\n",
        "\n",
        "sum_pool = SumPooling()\n",
        "avg_pool = AvgPooling()\n",
        "max_pool = MaxPooling()\n",
        "for i in train_idx:\n",
        "  emd = []\n",
        "  g = dataset[i][0]\n",
        "  l = dataset[i][1].detach().numpy()\n",
        "  e1 = GraphConv(3 ,  16, norm='both', weight=True, bias=True)\n",
        "  e1 = e1(g , g.ndata['attr'])\n",
        "  sum = sum_pool(g , e1)[0].detach().numpy()\n",
        "  avg = avg_pool(g , e1)[0].detach().numpy()\n",
        "  max = max_pool(g , e1)[0].detach().numpy()\n",
        "  emd.append(sum)\n",
        "  emd.append(avg)\n",
        "  emd.append(max)\n",
        "  e1 = SAGEConv(3,  16, 'pool')\n",
        "  e1 = e1(g , g.ndata['attr'])\n",
        "  sum = sum_pool(g , e1)[0].detach().numpy()\n",
        "  avg = avg_pool(g , e1)[0].detach().numpy()\n",
        "  max = max_pool(g , e1)[0].detach().numpy()\n",
        "  emd.append(sum)\n",
        "  emd.append(avg)\n",
        "  emd.append(max)\n",
        "  train_emd.append(emd)\n",
        "  train_label.append(l)\n",
        "\n",
        "for i in test_idx:\n",
        "  emd = []\n",
        "  g = dataset[i][0]\n",
        "  l = dataset[i][1].detach().numpy()\n",
        "  e1 = GraphConv(3 ,  16, norm='both', weight=True, bias=True)\n",
        "  e1 = e1(g , g.ndata['attr'])\n",
        "  sum = sum_pool(g , e1)[0].detach().numpy()\n",
        "  avg = avg_pool(g , e1)[0].detach().numpy()\n",
        "  max = max_pool(g , e1)[0].detach().numpy()\n",
        "  emd.append(sum)\n",
        "  emd.append(avg)\n",
        "  emd.append(max)\n",
        "  e1 = SAGEConv(3 ,  16,  'pool')\n",
        "  e1 = e1(g , g.ndata['attr'])\n",
        "  sum = sum_pool(g , e1)[0].detach().numpy()\n",
        "  avg = avg_pool(g , e1)[0].detach().numpy()\n",
        "  max = max_pool(g , e1)[0].detach().numpy()\n",
        "  emd.append(sum)\n",
        "  emd.append(avg)\n",
        "  emd.append(max)\n",
        "  test_emd.append(emd)\n",
        "  test_label.append(l)\n",
        "\n",
        "train_emd = np.array(train_emd)\n",
        "train_label = np.array(train_label)\n",
        "test_emd = np.array(test_emd)\n",
        "test_label = np.array(test_label)\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_emd = torch.tensor(train_emd)\n",
        "test_emd = torch.tensor(test_emd)\n",
        "train_label = torch.tensor(train_label)\n",
        "test_label = torch.tensor(test_label)\n",
        "\n",
        "print(train_emd.shape , ' ',train_label.shape)\n",
        "print(test_emd.shape , ' ',test_label.shape)\n",
        "\n",
        "train_data = TensorDataset(train_emd , train_label)\n",
        "test_data = TensorDataset(test_emd , test_label)\n",
        "\n",
        "train_loader = DataLoader(train_data , batch_size=16)\n",
        "\n",
        "test_loader = DataLoader(test_data , batch_size=16)\n",
        "\n",
        "\n",
        "net = Net(7 , 6 , 16 , 256 , 256 ,2 )\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "def train():\n",
        "  net.train()\n",
        "  for i , data in enumerate(train_loader):\n",
        "    x , l = data\n",
        "    optimizer.zero_grad()\n",
        "    out = net(x)\n",
        "    loss = criterion(out, l)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "  return loss.item()\n",
        "\n",
        "def test():\n",
        "  net.eval()\n",
        "\n",
        "  loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "      x , l = data\n",
        "      out = net(x)\n",
        "      loss += criterion(out, l).item()\n",
        "      pred = out.argmax(dim = 1, keepdim = True)\n",
        "      correct += pred.eq (l.view_as(pred)).sum().item()\n",
        "\n",
        "  return 100. * correct / len(test_emd) , loss / len(test_emd)\n",
        "\n",
        "\n",
        "for _ in range(1000):\n",
        "  print(train())\n",
        "  print(test())\n",
        "\n",
        "\n",
        "print('ACC AND LOSS' , test())\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1001, 6, 16])   torch.Size([1001])\n",
            "torch.Size([112, 6, 16])   torch.Size([112])\n",
            "0.8021624088287354\n",
            "(58.92857142857143, 0.04163598269224167)\n",
            "0.5562059879302979\n",
            "(76.78571428571429, 0.033701009516205103)\n",
            "0.5033747553825378\n",
            "(69.64285714285714, 0.03429496767265456)\n",
            "0.41816824674606323\n",
            "(70.53571428571429, 0.03404498392982142)\n",
            "0.444724977016449\n",
            "(69.64285714285714, 0.03672621798302446)\n",
            "0.3365400433540344\n",
            "(66.96428571428571, 0.03978177798645837)\n",
            "0.29932549595832825\n",
            "(69.64285714285714, 0.044087213064943044)\n",
            "0.10133260488510132\n",
            "(65.17857142857143, 0.051436668794069974)\n",
            "0.193223774433136\n",
            "(66.07142857142857, 0.051226728462747166)\n",
            "0.14550703763961792\n",
            "(66.96428571428571, 0.061222371246133535)\n",
            "0.23124490678310394\n",
            "(64.28571428571429, 0.07423170070563044)\n",
            "0.054603829979896545\n",
            "(69.64285714285714, 0.082057138638837)\n",
            "0.015208842232823372\n",
            "(68.75, 0.09235496446490288)\n",
            "0.16196811199188232\n",
            "(64.28571428571429, 0.06855205499700137)\n",
            "0.04414895921945572\n",
            "(62.5, 0.07212369410055024)\n",
            "0.009151019155979156\n",
            "(61.607142857142854, 0.10271071429763522)\n",
            "0.1550987958908081\n",
            "(63.392857142857146, 0.10892545325415474)\n",
            "0.03399502485990524\n",
            "(62.5, 0.09856705101472991)\n",
            "0.005384549964219332\n",
            "(62.5, 0.11486939021519252)\n",
            "0.007035363931208849\n",
            "(64.28571428571429, 0.1509774811565876)\n",
            "0.0002088135079247877\n",
            "(66.07142857142857, 0.15114453328507288)\n",
            "0.00019438839808572084\n",
            "(66.07142857142857, 0.16160354976143157)\n",
            "0.004755358211696148\n",
            "(68.75, 0.1023262148456914)\n",
            "0.005552260670810938\n",
            "(61.607142857142854, 0.11424152553081512)\n",
            "0.0030312538146972656\n",
            "(64.28571428571429, 0.13720406272581645)\n",
            "0.0011473414488136768\n",
            "(67.85714285714286, 0.16541516142232077)\n",
            "0.0017439412185922265\n",
            "(66.07142857142857, 0.1657254472374916)\n",
            "0.0009465325274504721\n",
            "(66.07142857142857, 0.17449967776026046)\n",
            "0.0005785534740425646\n",
            "(66.07142857142857, 0.17944290595395224)\n",
            "0.0004151990287937224\n",
            "(66.07142857142857, 0.1833840970482145)\n",
            "0.0003189779235981405\n",
            "(66.07142857142857, 0.1866964346596173)\n",
            "0.0002552830846980214\n",
            "(66.07142857142857, 0.18959407827683858)\n",
            "0.00021010299678891897\n",
            "(66.07142857142857, 0.19219204357692174)\n",
            "0.000176490459125489\n",
            "(66.07142857142857, 0.19456083327531815)\n",
            "0.0001506197586422786\n",
            "(66.07142857142857, 0.19674739560910634)\n",
            "0.000130154425278306\n",
            "(66.07142857142857, 0.19878507192645753)\n",
            "0.0001136154169216752\n",
            "(65.17857142857143, 0.20069857154573714)\n",
            "0.00010002517228713259\n",
            "(65.17857142857143, 0.2025067231484822)\n",
            "8.872299804352224e-05\n",
            "(65.17857142857143, 0.20422392232077463)\n",
            "7.916687900433317e-05\n",
            "(66.07142857142857, 0.20586201974323817)\n",
            "7.101308437995613e-05\n",
            "(66.07142857142857, 0.2074307490672384)\n",
            "6.402366852853447e-05\n",
            "(66.07142857142857, 0.2089378269655364)\n",
            "5.796052573714405e-05\n",
            "(66.07142857142857, 0.21038997173309326)\n",
            "5.2678180509246886e-05\n",
            "(66.07142857142857, 0.21179285326174327)\n",
            "4.801785325980745e-05\n",
            "(66.96428571428571, 0.21315113880804606)\n",
            "4.3900185119127855e-05\n",
            "(66.96428571428571, 0.21446894641433442)\n",
            "4.025904490845278e-05\n",
            "(66.96428571428571, 0.21574992473636354)\n",
            "3.6988531064707786e-05\n",
            "(66.96428571428571, 0.21699710935354233)\n",
            "3.408869088161737e-05\n",
            "(66.96428571428571, 0.21821332510028565)\n",
            "3.1480089091928676e-05\n",
            "(66.96428571428571, 0.21940109133720398)\n",
            "2.9123031708877534e-05\n",
            "(66.96428571428571, 0.2205623494727271)\n",
            "2.6991041522705927e-05\n",
            "(66.96428571428571, 0.22169926975454604)\n",
            "2.5031160475919023e-05\n",
            "(66.96428571428571, 0.2228134572505951)\n",
            "2.326988942513708e-05\n",
            "(66.96428571428571, 0.22390641484941756)\n",
            "2.168075297959149e-05\n",
            "(66.96428571428571, 0.22497976145574025)\n",
            "2.019753446802497e-05\n",
            "(66.96428571428571, 0.2260347604751587)\n",
            "1.885997517092619e-05\n",
            "(66.96428571428571, 0.22707244221653258)\n",
            "1.7628344721742906e-05\n",
            "(66.96428571428571, 0.22809402538197382)\n",
            "1.650265221542213e-05\n",
            "(66.96428571428571, 0.22910030292613165)\n",
            "1.5456409528269432e-05\n",
            "(66.96428571428571, 0.23009238498551504)\n",
            "1.447637168894289e-05\n",
            "(66.96428571428571, 0.23107092933995382)\n",
            "1.3589035916083958e-05\n",
            "(66.96428571428571, 0.23203681941543305)\n",
            "1.2754670024150982e-05\n",
            "(66.96428571428571, 0.23299070128372737)\n",
            "1.1999760317848995e-05\n",
            "(66.96428571428571, 0.23393313586711884)\n",
            "1.1284578249615151e-05\n",
            "(66.96428571428571, 0.23486497146742685)\n",
            "1.0622369700286072e-05\n",
            "(66.96428571428571, 0.235786572098732)\n",
            "9.999888789025135e-06\n",
            "(66.96428571428571, 0.2366984337568283)\n",
            "9.430382306163665e-06\n",
            "(66.96428571428571, 0.23760112481457846)\n",
            "8.900607099349145e-06\n",
            "(66.96428571428571, 0.23849490604230336)\n",
            "8.384072316403035e-06\n",
            "(66.96428571428571, 0.23938060339008058)\n",
            "7.920514690340497e-06\n",
            "(66.96428571428571, 0.24025829136371613)\n",
            "7.48344382373034e-06\n",
            "(66.96428571428571, 0.2411285170486995)\n",
            "7.086106052156538e-06\n",
            "(66.96428571428571, 0.24199144861527852)\n",
            "6.702010523440549e-06\n",
            "(66.96428571428571, 0.2428476427282606)\n",
            "6.344404027913697e-06\n",
            "(66.96428571428571, 0.24369728139468602)\n",
            "6.013285201333929e-06\n",
            "(66.96428571428571, 0.24454071053436824)\n",
            "5.695410436601378e-06\n",
            "(66.96428571428571, 0.24537829416138784)\n",
            "5.390778824221343e-06\n",
            "(66.96428571428571, 0.24620990668024337)\n",
            "5.112637154525146e-06\n",
            "(66.96428571428571, 0.24703621438571385)\n",
            "4.847739546676166e-06\n",
            "(66.96428571428571, 0.24785712574209487)\n",
            "4.596086000674404e-06\n",
            "(66.96428571428571, 0.24867301753589086)\n",
            "4.3709219426091295e-06\n",
            "(66.96428571428571, 0.24948414308684214)\n",
            "4.145757429796504e-06\n",
            "(66.96428571428571, 0.25029072165489197)\n",
            "3.947082404920366e-06\n",
            "(66.96428571428571, 0.25109285541943144)\n",
            "3.748407152670552e-06\n",
            "(66.96428571428571, 0.25189071893692017)\n",
            "3.562976644388982e-06\n",
            "(66.96428571428571, 0.25268426537513733)\n",
            "3.3907906527019804e-06\n",
            "(66.96428571428571, 0.25347403969083515)\n",
            "3.2186044336413033e-06\n",
            "(66.96428571428571, 0.254259786435536)\n",
            "3.0729086120118154e-06\n",
            "(66.96428571428571, 0.2550420420510428)\n",
            "2.927212108261301e-06\n",
            "(66.96428571428571, 0.25582054257392883)\n",
            "2.781515604510787e-06\n",
            "(66.96428571428571, 0.256595749940191)\n",
            "2.649064072102192e-06\n",
            "(66.96428571428571, 0.2573677109820502)\n",
            "2.529857511035516e-06\n",
            "(66.96428571428571, 0.2581365087202617)\n",
            "2.41065094996884e-06\n",
            "(66.96428571428571, 0.25890228152275085)\n",
            "2.291443934154813e-06\n",
            "(66.96428571428571, 0.2596651613712311)\n",
            "2.1854823444300564e-06\n",
            "(66.96428571428571, 0.26042508653232027)\n",
            "2.079520527331624e-06\n",
            "(66.96428571428571, 0.2611823316131319)\n",
            "1.9868039089487866e-06\n",
            "(66.96428571428571, 0.26193675824574064)\n",
            "1.8808417507898412e-06\n",
            "(66.96428571428571, 0.26268855375902994)\n",
            "1.8013702174357604e-06\n",
            "(66.96428571428571, 0.26343810132571627)\n",
            "1.721898570394842e-06\n",
            "(66.96428571428571, 0.26418535837105345)\n",
            "1.6424269233539235e-06\n",
            "(66.96428571428571, 0.26493024613176075)\n",
            "1.562955276313005e-06\n",
            "(66.96428571428571, 0.2656728412423815)\n",
            "1.4967287143008434e-06\n",
            "(66.96428571428571, 0.26641322885240826)\n",
            "1.4305022659755195e-06\n",
            "(66.96428571428571, 0.2671517942632948)\n",
            "1.364275703963358e-06\n",
            "(66.96428571428571, 0.26788849915776936)\n",
            "1.2980491419511964e-06\n",
            "(66.96428571428571, 0.26862333927835735)\n",
            "1.2450677786546294e-06\n",
            "(66.96428571428571, 0.26935602724552155)\n",
            "1.1920865290449e-06\n",
            "(66.96428571428571, 0.2700869653906141)\n",
            "1.139105165748333e-06\n",
            "(66.96428571428571, 0.27081596638475147)\n",
            "1.0861236887649284e-06\n",
            "(66.96428571428571, 0.27154370503766195)\n",
            "1.0331423254683614e-06\n",
            "(66.96428571428571, 0.27226994718824116)\n",
            "9.934062745742267e-07\n",
            "(66.96428571428571, 0.27299446293285917)\n",
            "9.536702805235109e-07\n",
            "(66.96428571428571, 0.2737172416278294)\n",
            "9.006887466966873e-07\n",
            "(66.96428571428571, 0.27443877714020865)\n",
            "8.609526389591338e-07\n",
            "(66.96428571428571, 0.27515874803066254)\n",
            "8.344619004674314e-07\n",
            "(66.96428571428571, 0.2758772075176239)\n",
            "7.947257927298779e-07\n",
            "(66.96428571428571, 0.2765941896608898)\n",
            "7.549895713054866e-07\n",
            "(66.96428571428571, 0.2773096816880362)\n",
            "7.284988328137842e-07\n",
            "(66.96428571428571, 0.2780232621090753)\n",
            "7.02008037478663e-07\n",
            "(66.96428571428571, 0.27873550781181883)\n",
            "6.622718728976906e-07\n",
            "(66.96428571428571, 0.27944679984024595)\n",
            "6.357810775625694e-07\n",
            "(66.96428571428571, 0.2801569125482014)\n",
            "6.092902822274482e-07\n",
            "(66.96428571428571, 0.2808655457837241)\n",
            "5.827994300489081e-07\n",
            "(66.96428571428571, 0.2815729422228677)\n",
            "5.56308577870368e-07\n",
            "(66.96428571428571, 0.2822799725191934)\n",
            "5.298177825352468e-07\n",
            "(66.96428571428571, 0.2829858383962086)\n",
            "5.165723564459768e-07\n",
            "(66.96428571428571, 0.2836906739643642)\n",
            "4.900815611108555e-07\n",
            "(66.96428571428571, 0.2843946005616869)\n",
            "4.768361350215855e-07\n",
            "(66.96428571428571, 0.2850977395262037)\n",
            "4.503452828430454e-07\n",
            "(66.96428571428571, 0.2858001696211951)\n",
            "4.370998567537754e-07\n",
            "(66.96428571428571, 0.28650143316813875)\n",
            "4.106090045752353e-07\n",
            "(66.96428571428571, 0.2872015791279929)\n",
            "3.9736357848596526e-07\n",
            "(66.96428571428571, 0.28790066497666494)\n",
            "3.841181523966952e-07\n",
            "(66.96428571428571, 0.2885985864060266)\n",
            "3.708727263074252e-07\n",
            "(66.96428571428571, 0.2892957947083882)\n",
            "3.443818741288851e-07\n",
            "(66.96428571428571, 0.28999174918447224)\n",
            "3.3113641961790563e-07\n",
            "(66.96428571428571, 0.2906875099454607)\n",
            "3.178909935286356e-07\n",
            "(66.96428571428571, 0.2913817103419985)\n",
            "3.046455390176561e-07\n",
            "(66.96428571428571, 0.29207615767206463)\n",
            "2.914001129283861e-07\n",
            "(66.96428571428571, 0.2927691638469696)\n",
            "2.781546584174066e-07\n",
            "(66.96428571428571, 0.29346146966729847)\n",
            "2.781546584174066e-07\n",
            "(66.96428571428571, 0.2941527600799288)\n",
            "2.6490923232813657e-07\n",
            "(66.96428571428571, 0.29484304785728455)\n",
            "2.516637778171571e-07\n",
            "(66.96428571428571, 0.29553249904087614)\n",
            "2.3841832330617763e-07\n",
            "(66.96428571428571, 0.2962221120085035)\n",
            "2.2517286879519816e-07\n",
            "(66.96428571428571, 0.2969107223408563)\n",
            "2.2517286879519816e-07\n",
            "(66.96428571428571, 0.29759894950049265)\n",
            "2.119274284950734e-07\n",
            "(66.96428571428571, 0.29828608461788725)\n",
            "1.9868197398409393e-07\n",
            "(66.96428571428571, 0.29897261943135944)\n",
            "1.9868197398409393e-07\n",
            "(66.96428571428571, 0.29965880300317493)\n",
            "1.8543651947311446e-07\n",
            "(66.96428571428571, 0.3003442244870322)\n",
            "1.8543651947311446e-07\n",
            "(66.96428571428571, 0.30102740015302387)\n",
            "1.7219105075128027e-07\n",
            "(66.96428571428571, 0.3017095795699528)\n",
            "1.5894561045115552e-07\n",
            "(66.96428571428571, 0.30239107991967884)\n",
            "1.5894561045115552e-07\n",
            "(66.96428571428571, 0.30307406399931225)\n",
            "1.5894561045115552e-07\n",
            "(66.96428571428571, 0.3037566840648651)\n",
            "1.4570014172932133e-07\n",
            "(66.96428571428571, 0.30443931903157917)\n",
            "1.4570014172932133e-07\n",
            "(66.96428571428571, 0.3051220774650574)\n",
            "1.3245468721834186e-07\n",
            "(66.96428571428571, 0.30580395673002514)\n",
            "1.3245468721834186e-07\n",
            "(66.96428571428571, 0.3064838818141392)\n",
            "1.1920922560193503e-07\n",
            "(66.96428571428571, 0.30716297243322643)\n",
            "1.1920922560193503e-07\n",
            "(66.96428571428571, 0.30784306143011364)\n",
            "1.1920922560193503e-07\n",
            "(66.96428571428571, 0.3085202638592039)\n",
            "1.059637639855282e-07\n",
            "(66.96428571428571, 0.30919824114867617)\n",
            "1.059637639855282e-07\n",
            "(66.96428571428571, 0.3098763631922858)\n",
            "1.059637639855282e-07\n",
            "(66.96428571428571, 0.31055220322949545)\n",
            "9.271829526369402e-08\n",
            "(66.96428571428571, 0.3112312065703528)\n",
            "9.271829526369402e-08\n",
            "(66.96428571428571, 0.3119076064654759)\n",
            "9.271829526369402e-08\n",
            "(66.96428571428571, 0.3125855049916676)\n",
            "9.271829526369402e-08\n",
            "(66.96428571428571, 0.3132634631225041)\n",
            "7.947283364728719e-08\n",
            "(66.96428571428571, 0.3139387433018003)\n",
            "7.947283364728719e-08\n",
            "(66.96428571428571, 0.31461196286337717)\n",
            "7.947283364728719e-08\n",
            "(66.96428571428571, 0.3152846630130495)\n",
            "7.947283364728719e-08\n",
            "(66.96428571428571, 0.31595861273152487)\n",
            "6.6227364925453e-08\n",
            "(66.96428571428571, 0.31663259650979725)\n",
            "6.6227364925453e-08\n",
            "(66.96428571428571, 0.3173036298581532)\n",
            "6.6227364925453e-08\n",
            "(66.96428571428571, 0.3179773283856256)\n",
            "6.6227364925453e-08\n",
            "(66.96428571428571, 0.31865267029830385)\n",
            "6.6227364925453e-08\n",
            "(66.96428571428571, 0.3193236951317106)\n",
            "5.298189265090514e-08\n",
            "(66.96428571428571, 0.3199941856520517)\n",
            "5.298189265090514e-08\n",
            "(66.96428571428571, 0.3206628518445151)\n",
            "5.298189265090514e-08\n",
            "(66.96428571428571, 0.32133119446890696)\n",
            "5.298189265090514e-08\n",
            "(66.96428571428571, 0.32199908580098835)\n",
            "5.298189265090514e-08\n",
            "(66.96428571428571, 0.3226656722170966)\n",
            "5.298189265090514e-08\n",
            "(66.96428571428571, 0.3233334741422108)\n",
            "3.973642392907095e-08\n",
            "(66.96428571428571, 0.3239970420088087)\n",
            "3.973642392907095e-08\n",
            "(66.96428571428571, 0.3246583193540573)\n",
            "3.973642392907095e-08\n",
            "(66.96428571428571, 0.32531794692788807)\n",
            "3.973642392907095e-08\n",
            "(66.96428571428571, 0.32597719558647703)\n",
            "3.973642392907095e-08\n",
            "(66.96428571428571, 0.3266307158129556)\n",
            "3.973642392907095e-08\n",
            "(66.96428571428571, 0.3272874014718192)\n",
            "3.973642392907095e-08\n",
            "(66.96428571428571, 0.3279469055788858)\n",
            "3.973642392907095e-08\n",
            "(66.96428571428571, 0.32860624364444185)\n",
            "3.973642392907095e-08\n",
            "(66.96428571428571, 0.3292675146034786)\n",
            "2.6490949878166248e-08\n",
            "(66.96428571428571, 0.32993328145572115)\n",
            "2.6490949878166248e-08\n",
            "(66.96428571428571, 0.33059732615947723)\n",
            "2.6490949878166248e-08\n",
            "(66.96428571428571, 0.3312621755259378)\n",
            "2.6490949878166248e-08\n",
            "(66.96428571428571, 0.3319205492734909)\n",
            "2.6490949878166248e-08\n",
            "(66.96428571428571, 0.33258206503731863)\n",
            "2.6490949878166248e-08\n",
            "(66.96428571428571, 0.333242552621024)\n",
            "2.6490949878166248e-08\n",
            "(66.96428571428571, 0.3339015671185085)\n",
            "2.6490949878166248e-08\n",
            "(66.96428571428571, 0.33456422814301084)\n",
            "2.6490949878166248e-08\n",
            "(66.96428571428571, 0.3352230872426714)\n",
            "2.6490949878166248e-08\n",
            "(66.96428571428571, 0.3358841027532305)\n",
            "2.6490949878166248e-08\n",
            "(66.96428571428571, 0.3365485114710672)\n",
            "2.6490949878166248e-08\n",
            "(66.96428571428571, 0.33720114614282337)\n",
            "2.6490949878166248e-08\n",
            "(66.96428571428571, 0.33785365309034077)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.33849781113011496)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.3391435742378235)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.33978954596178873)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.34043510045324055)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.34108469103063854)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.34173212732587543)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.34237644927842276)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.34302158440862385)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.3436656253678458)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.3443098004375185)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.3449566960334778)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.34559611124651773)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.3462228349276951)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.34685084223747253)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.347480839916638)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.3481105480875288)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.348742459501539)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.3493735258068357)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.35000544999326977)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.35064265770571573)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.3512808808258602)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.3519237701381956)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.3525626872267042)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.35320540411131723)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.3538486659526825)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.35448761710098814)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.35513295446123394)\n",
            "1.3245475827261544e-08\n",
            "(66.96428571428571, 0.35576147053922924)\n",
            "0.0\n",
            "(66.96428571428571, 0.3563880366938455)\n",
            "0.0\n",
            "(66.96428571428571, 0.357013219169208)\n",
            "0.0\n",
            "(66.96428571428571, 0.3576415010860988)\n",
            "0.0\n",
            "(66.96428571428571, 0.3582735742841448)\n",
            "0.0\n",
            "(66.96428571428571, 0.35890723552022663)\n",
            "0.0\n",
            "(66.96428571428571, 0.35954315960407257)\n",
            "0.0\n",
            "(66.96428571428571, 0.3601782832826887)\n",
            "0.0\n",
            "(66.96428571428571, 0.36081519084317343)\n",
            "0.0\n",
            "(66.96428571428571, 0.36144155476774487)\n",
            "0.0\n",
            "(66.96428571428571, 0.3620701176779611)\n",
            "0.0\n",
            "(66.96428571428571, 0.3626917174884251)\n",
            "0.0\n",
            "(66.96428571428571, 0.36330405942031313)\n",
            "0.0\n",
            "(66.96428571428571, 0.3639101449932371)\n",
            "0.0\n",
            "(66.96428571428571, 0.3645002671650478)\n",
            "0.0\n",
            "(66.96428571428571, 0.3650910769190107)\n",
            "0.0\n",
            "(66.96428571428571, 0.3656867508377348)\n",
            "0.0\n",
            "(66.96428571428571, 0.3662843831947872)\n",
            "0.0\n",
            "(66.96428571428571, 0.36688499578407835)\n",
            "0.0\n",
            "(66.96428571428571, 0.36748719215393066)\n",
            "0.0\n",
            "(66.96428571428571, 0.3680777166570936)\n",
            "0.0\n",
            "(66.96428571428571, 0.3686755469867161)\n",
            "0.0\n",
            "(66.96428571428571, 0.3692652923720224)\n",
            "0.0\n",
            "(66.96428571428571, 0.369848370552063)\n",
            "0.0\n",
            "(66.96428571428571, 0.37043267701353344)\n",
            "0.0\n",
            "(66.96428571428571, 0.37103936289037975)\n",
            "0.0\n",
            "(66.96428571428571, 0.37165035946028574)\n",
            "0.0\n",
            "(66.96428571428571, 0.3722622458423887)\n",
            "0.0\n",
            "(66.96428571428571, 0.3728580410991396)\n",
            "0.0\n",
            "(66.96428571428571, 0.37344518090997425)\n",
            "0.0\n",
            "(66.96428571428571, 0.37400640760149273)\n",
            "0.0\n",
            "(66.96428571428571, 0.37456148862838745)\n",
            "0.0\n",
            "(66.96428571428571, 0.3751037120819092)\n",
            "0.0\n",
            "(66.96428571428571, 0.37561312104974476)\n",
            "0.0\n",
            "(66.96428571428571, 0.37612747507435934)\n",
            "0.0\n",
            "(66.96428571428571, 0.3766623671565737)\n",
            "0.0\n",
            "(66.96428571428571, 0.37718282427106586)\n",
            "0.0\n",
            "(66.96428571428571, 0.37769606496606556)\n",
            "0.0\n",
            "(66.96428571428571, 0.378199360200337)\n",
            "0.0\n",
            "(66.96428571428571, 0.37867386639118195)\n",
            "0.0\n",
            "(66.96428571428571, 0.37911943239825113)\n",
            "0.0\n",
            "(66.96428571428571, 0.3795652027641024)\n",
            "0.0\n",
            "(66.96428571428571, 0.3800245864050729)\n",
            "0.0\n",
            "(66.96428571428571, 0.3804904520511627)\n",
            "0.0\n",
            "(66.96428571428571, 0.38096053898334503)\n",
            "0.0\n",
            "(66.96428571428571, 0.381430127790996)\n",
            "0.0\n",
            "(66.96428571428571, 0.38190594954150064)\n",
            "0.0\n",
            "(66.96428571428571, 0.3823879744325365)\n",
            "0.0\n",
            "(66.96428571428571, 0.3828669381993158)\n",
            "0.0\n",
            "(66.96428571428571, 0.38335112375872477)\n",
            "0.0\n",
            "(66.96428571428571, 0.3838259492601667)\n",
            "0.0\n",
            "(66.96428571428571, 0.38430517486163546)\n",
            "0.0\n",
            "(66.96428571428571, 0.38478370223726543)\n",
            "0.0\n",
            "(66.96428571428571, 0.3852733905826296)\n",
            "0.0\n",
            "(66.96428571428571, 0.385776549577713)\n",
            "0.0\n",
            "(66.96428571428571, 0.38626436037676676)\n",
            "0.0\n",
            "(66.96428571428571, 0.38675313123634886)\n",
            "0.0\n",
            "(66.96428571428571, 0.3872186818293163)\n",
            "0.0\n",
            "(66.96428571428571, 0.3876980606998716)\n",
            "0.0\n",
            "(66.96428571428571, 0.38817931711673737)\n",
            "0.0\n",
            "(66.96428571428571, 0.38866592943668365)\n",
            "0.0\n",
            "(66.96428571428571, 0.38912689685821533)\n",
            "0.0\n",
            "(66.96428571428571, 0.38956289419106077)\n",
            "0.0\n",
            "(66.96428571428571, 0.39000404519694193)\n",
            "0.0\n",
            "(66.96428571428571, 0.39044903431619915)\n",
            "0.0\n",
            "(66.96428571428571, 0.39089752095086233)\n",
            "0.0\n",
            "(66.96428571428571, 0.3913498818874359)\n",
            "0.0\n",
            "(66.96428571428571, 0.3918055977140154)\n",
            "0.0\n",
            "(66.96428571428571, 0.3922651388815471)\n",
            "0.0\n",
            "(66.96428571428571, 0.39272813498973846)\n",
            "0.0\n",
            "(66.96428571428571, 0.3931944966316223)\n",
            "0.0\n",
            "(66.96428571428571, 0.3936642642532076)\n",
            "0.0\n",
            "(66.96428571428571, 0.3941374399832317)\n",
            "0.0\n",
            "(66.96428571428571, 0.39461391951356617)\n",
            "0.0\n",
            "(66.96428571428571, 0.3950886258057186)\n",
            "0.0\n",
            "(66.96428571428571, 0.3955666699579784)\n",
            "0.0\n",
            "(66.96428571428571, 0.39604783058166504)\n",
            "0.0\n",
            "(66.96428571428571, 0.3965323269367218)\n",
            "0.0\n",
            "(66.96428571428571, 0.3970197630780084)\n",
            "0.0\n",
            "(66.96428571428571, 0.39750557925019947)\n",
            "0.0\n",
            "(66.96428571428571, 0.3979923980576651)\n",
            "0.0\n",
            "(66.96428571428571, 0.398482152393886)\n",
            "0.0\n",
            "(66.96428571428571, 0.3989745186907904)\n",
            "0.0\n",
            "(66.96428571428571, 0.3994694437299456)\n",
            "0.0\n",
            "(66.96428571428571, 0.399967080780438)\n",
            "0.0\n",
            "(66.96428571428571, 0.4004672190972737)\n",
            "0.0\n",
            "(66.96428571428571, 0.4009695989745004)\n",
            "0.0\n",
            "(66.96428571428571, 0.4014744205134256)\n",
            "0.0\n",
            "(66.96428571428571, 0.4019814410379955)\n",
            "0.0\n",
            "(66.96428571428571, 0.40249052005154745)\n",
            "0.0\n",
            "(66.96428571428571, 0.4030018725565502)\n",
            "0.0\n",
            "(66.96428571428571, 0.40351506216185434)\n",
            "0.0\n",
            "(66.96428571428571, 0.4040303294147764)\n",
            "0.0\n",
            "(66.96428571428571, 0.4045474891151701)\n",
            "0.0\n",
            "(66.96428571428571, 0.40506637947899954)\n",
            "0.0\n",
            "(66.96428571428571, 0.40558691535677227)\n",
            "0.0\n",
            "(66.96428571428571, 0.40610912655081066)\n",
            "0.0\n",
            "(66.96428571428571, 0.40663287682192667)\n",
            "0.0\n",
            "(66.96428571428571, 0.40715823003223967)\n",
            "0.0\n",
            "(66.96428571428571, 0.40768514786447796)\n",
            "0.0\n",
            "(66.96428571428571, 0.4082134131874357)\n",
            "0.0\n",
            "(66.96428571428571, 0.40874297491141726)\n",
            "0.0\n",
            "(66.96428571428571, 0.40927368828228544)\n",
            "0.0\n",
            "(66.96428571428571, 0.4098057917186192)\n",
            "0.0\n",
            "(66.96428571428571, 0.4103236624172756)\n",
            "0.0\n",
            "(66.96428571428571, 0.41084263154438566)\n",
            "0.0\n",
            "(66.96428571428571, 0.4113632334130151)\n",
            "0.0\n",
            "(66.96428571428571, 0.4118849060365132)\n",
            "0.0\n",
            "(66.96428571428571, 0.4124079793691635)\n",
            "0.0\n",
            "(66.96428571428571, 0.41293210429804666)\n",
            "0.0\n",
            "(66.96428571428571, 0.4134576065199716)\n",
            "0.0\n",
            "(66.96428571428571, 0.41398410499095917)\n",
            "0.0\n",
            "(66.96428571428571, 0.41451183600085123)\n",
            "0.0\n",
            "(66.96428571428571, 0.41504037593092236)\n",
            "0.0\n",
            "(66.96428571428571, 0.41556989933763233)\n",
            "0.0\n",
            "(66.96428571428571, 0.41610009542533327)\n",
            "0.0\n",
            "(66.96428571428571, 0.4166311983551298)\n",
            "0.0\n",
            "(66.96428571428571, 0.41716285262789043)\n",
            "0.0\n",
            "(66.96428571428571, 0.4176951902253287)\n",
            "0.0\n",
            "(66.96428571428571, 0.41822818560259684)\n",
            "0.0\n",
            "(66.96428571428571, 0.4187616045985903)\n",
            "0.0\n",
            "(66.96428571428571, 0.4192954195397241)\n",
            "0.0\n",
            "(66.96428571428571, 0.4198295452765056)\n",
            "0.0\n",
            "(66.96428571428571, 0.42036397542272297)\n",
            "0.0\n",
            "(66.96428571428571, 0.4208987184933254)\n",
            "0.0\n",
            "(66.96428571428571, 0.4214335637433188)\n",
            "0.0\n",
            "(66.96428571428571, 0.4219684898853302)\n",
            "0.0\n",
            "(66.96428571428571, 0.42250366508960724)\n",
            "0.0\n",
            "(66.96428571428571, 0.42303868915353504)\n",
            "0.0\n",
            "(66.96428571428571, 0.42357364084039417)\n",
            "0.0\n",
            "(66.96428571428571, 0.4241084158420563)\n",
            "0.0\n",
            "(66.96428571428571, 0.42464318232876913)\n",
            "0.0\n",
            "(66.96428571428571, 0.42517765930720736)\n",
            "0.0\n",
            "(66.96428571428571, 0.4257118936095919)\n",
            "0.0\n",
            "(66.96428571428571, 0.4262455105781555)\n",
            "0.0\n",
            "(66.96428571428571, 0.4267789529902594)\n",
            "0.0\n",
            "(66.96428571428571, 0.4273121505975723)\n",
            "0.0\n",
            "(66.96428571428571, 0.42784469042505535)\n",
            "0.0\n",
            "(66.96428571428571, 0.42837661717619213)\n",
            "0.0\n",
            "(66.96428571428571, 0.42890818629946026)\n",
            "0.0\n",
            "(66.96428571428571, 0.42943901249340605)\n",
            "0.0\n",
            "(66.96428571428571, 0.4299691936799458)\n",
            "0.0\n",
            "(66.96428571428571, 0.4304988171373095)\n",
            "0.0\n",
            "(66.96428571428571, 0.43102759761469706)\n",
            "0.0\n",
            "(66.96428571428571, 0.4315556160041264)\n",
            "0.0\n",
            "(66.96428571428571, 0.4320829893861498)\n",
            "0.0\n",
            "(66.96428571428571, 0.43260943038122995)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-296cee40fbdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-296cee40fbdd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-296cee40fbdd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m       \u001b[0mpooled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-296cee40fbdd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_mixing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannel_mixing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-296cee40fbdd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \"\"\"\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mgelu\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m   1457\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_unary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1459\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}